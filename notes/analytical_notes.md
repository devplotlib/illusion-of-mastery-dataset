# Analytical Notes

These notes capture early observations, hypotheses, and unresolved questions emerging from the first pass through the dataset. They are intentionally informal and exploratory.

---

## 1. Emerging Patterns

- Confidence remains strangely stable even when accuracy fluctuates. A learner can be completely wrong on a structurally complex problem but still mark themselves as “4/5 confident.” This stability is surprising; I expected confidence to drop after an incorrect attempt.

- AI-assisted attempts look polished regardless of conceptual quality. The phrasing almost “fakes” competence, and learners seem to adopt this tone even when the underlying reasoning is brittle.

- Transfer failure is most common exactly where the tasks *look* similar on the surface. This suggests learners are cueing off superficial familiarity rather than structural understanding.

- When feedback is immediate, revisions improve procedurally but almost never conceptually. Delayed feedback creates more varied revisions, some better, some worse, but there’s slightly more reflection.

---

## 2. Hypotheses Under Consideration

- **Hypothesis:** Learners use linguistic fluency (their own or AI-generated) as a proxy for correctness.  
  **Support:** High confidence paired with obviously flawed reasoning, especially in AI-assisted contexts.  
  **Counterpoint:** Some learners show the same pattern even without AI. So fluency may be a general metacognitive shortcut.

- **Hypothesis:** Conceptual misconceptions act like “anchors.” Once a learner forms a simplified rule (e.g., “differentiate each part separately”), they apply it everywhere unless explicitly confronted.  
  **Support:** Misapplications appear repeatedly across timepoints—even after corrections.

- **Hypothesis:** Confidence may be more predictive of *task familiarity* than task correctness.  
  **Support:** Confidence rises on problems structurally similar to earlier ones, even when learners still get the reasoning wrong.

---

## 3. Surprising Observations

- Some revisions get *worse* after feedback. Not dramatically worse, but just “off” in new ways—almost like learners misinterpret the hint and create a novel error. This is not something I expected to see this early.

- A few learners seem to "mimic" AI reasoning style even when working without AI. The tone and structure of their explanations look machine-like. Possible internalization of AI patterns?

- In some sessions, learners switch from confident to highly uncertain with no change in task difficulty. Possible fatigue effect, or maybe a failed attempt at self-correction.

- One interesting pattern: when learners receive delayed feedback, they sometimes pre-emptively revise their reasoning before seeing any feedback, almost as if they grow suspicious of their own initial answers.

---

## 4. Ambiguities to Resolve

- The line between “overgeneralization” and “misapplied prior rule” can blur. Both involve inappropriate extrapolation. I need clearer heuristics for distinguishing them reliably.

- Confidence ratings: are some learners using the scale differently? A minority never drop below 3, which could reflect personality rather than metacognition.

- AI usage degree: detecting partial AI influence is messy unless explicit metadata exists. May need a stricter set of linguistic heuristics.

---

## 5. Next Analytical Steps

- **Cluster analysis:** Examine whether error types co-occur in predictable patterns. I suspect “misapplied prior rule” + “surface-level mimicry” is a real pair.

- **Longitudinal curves:** Plot confidence vs. accuracy over time for individual learners to see whether divergence grows or shrinks with repeated exposure.

- **Feedback interaction:** Compare whether the *timing* or the *source* (AI vs human) has a stronger effect on revision quality.

- **Linguistic markers:** Develop a small lexicon of high-confidence vs low-confidence language to classify confidence behaviorally when self-reports are absent.

---

End of analytical notes.
